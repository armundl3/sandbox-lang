# Configuration for CLI Chat App
model_name: "gemma:2b"  # Fast, small model for local use
model_provider: "ollama"
base_url: "http://localhost:11434"
keep_alive: -1  # Keep model loaded in memory
streaming: true
history_turns: 4  # Number of conversation turns to keep in memory
timeout: 30  # Request timeout in seconds

# System prompt
system_prompt: |
  You are a helpful assistant. Reply to user queries in a clear and informative manner.

# Model parameters
temperature: 0.7
max_tokens: 1024