{
  "name": "local-llm-chat",
  "private": true,
  "version": "0.1.0",
  "description": "ChatGPT-like interface for local Ollama models",
  "scripts": {
    "dev": "concurrently \"npm run dev:backend\" \"npm run dev:frontend\"",
    "dev:backend": "cd backend && uv run uvicorn app.main:app --reload --port 8000",
    "dev:frontend": "cd frontend && npm run dev",
    "setup": "npm install && npm run setup:backend && npm run setup:frontend",
    "setup:backend": "uv sync",
    "setup:frontend": "cd frontend && npm install",
    "build": "cd frontend && npm run build",
    "build:backend": "echo 'No build step needed for backend'",
    "test": "echo 'No tests configured yet'",
    "lint": "cd frontend && npm run lint"
  },
  "devDependencies": {
    "concurrently": "^8.2.0"
  },
  "keywords": ["llm", "chat", "ollama", "fastapi", "react"],
  "author": "sandbox-lang",
  "license": "MIT"
}